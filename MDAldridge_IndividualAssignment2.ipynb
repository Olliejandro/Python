{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDAldridge_IndividualAssignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mialdrid/Python/blob/master/MDAldridge_IndividualAssignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tof8VN4S-vZj",
        "colab_type": "text"
      },
      "source": [
        "Michael Aldridge\n",
        "Individual Assignment 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS1pS705srk-",
        "colab_type": "code",
        "outputId": "f28bdd64-966a-4852-b271-655777fcc9df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# Import package\n",
        "import nltk\n",
        "nltk.download('punkt') # tokenizer\n",
        "nltk.download('wordnet') # lemmatizer\n",
        "nltk.download('stopwords') # used to handle words like a, an, the\n",
        "nltk.download('averaged_perceptron_tagger') # Part of Speech\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7e-aL5y66g5",
        "colab_type": "code",
        "outputId": "d64bd0a6-6258-49a8-9ff1-ba480a4757a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# Store GitHub link to Excel sheet as a variable\n",
        "GitHubUrl= 'https://raw.githubusercontent.com/mialdrid/Python/master/IA2.csv'\n",
        "# Read csv file into dataframe \n",
        "df = pd.read_csv(GitHubUrl,header=None)\n",
        "df.columns = ['index', 'comment']\n",
        "#head df to view results\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>comment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>This is the place to go after this branch to t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The direct delivery from Star Theater in Chian...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>It is very delicious food. Good taste Unspoilt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>This house is the second time. I used to be a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Featured is majestic. Located on the road 107 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   index                                            comment\n",
              "0      1  This is the place to go after this branch to t...\n",
              "1      2  The direct delivery from Star Theater in Chian...\n",
              "2      3  It is very delicious food. Good taste Unspoilt...\n",
              "3      4  This house is the second time. I used to be a ...\n",
              "4      5  Featured is majestic. Located on the road 107 ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtbqROO87uZG",
        "colab_type": "code",
        "outputId": "d0d30dff-85f2-4c13-a0e8-e430065b898b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#Tokenize each comment/row\n",
        "for row in df.comment:\n",
        "  comments = nltk.word_tokenize(row)\n",
        "\n",
        "print(comments)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Some', 'of', 'Liu', 'Zhenyun', '’', 's', 'works', 'have', 'been', 'read', 'intermittently', '.', 'The', 'small', 'forest', 'in', 'the', '“', 'Unit', '”', 'and', '“', 'Chicken', 'of', 'the', 'Land', '”', 'grew', 'up', 'under', 'the', 'reform', 'and', 'opening', 'up', 'and', 'the', 'market', 'economy', '.', 'Many', 'historical', 'figures', 'in', 'the', '“', 'Hometown', '”', 'series', 'encircle', 'the', 'historical', 'reality', ',', 'nothing', 'more', 'than', 'Liu', 'Zhenyun', '’', 's', 'black', 'humor', '.', 'The', 'language', 'reveals', 'his', 'embarrassment', 'as', 'a', 'writer', '.', 'The', 'characters', 'are', 'not', 'important', '.', 'What', 'is', 'important', 'is', 'to', 'dispel', 'the', 'grand', 'narrative', 'in', 'the', 'historical', 'context', ',', 'to', 'defeat', 'the', 'tall', 'and', 'complete', ',', 'and', 'to', 'return', 'a', 'real', 'name', 'to', 'the', 'world', '.', 'In', 'Liu', 'Zhenyun', '&', '#', '39', ';', 's', 'works', ',', 'you', 'can', 'almost', 'see', 'too', 'many', 'postmodernist', 'techniques', ',', 'rather', 'than', 'the', 'pure', 'modernity', 'of', 'the', 'sorrowful', ',', 'magical', 'realism', ',', 'in', 'the', '&', 'quot', ';', 'hometown', 'circulate', '&', 'quot', ';', ',', 'the', 'six', 'fingers', 'of', 'the', 'excess', 'fingers', 'set', 'up', 'a', 'bridge', ',', 'let', 'Hundreds', 'of', 'thousands', 'of', 'immigrant', 'Yanjin', 'people', 'have', 'successfully', 'crossed', 'the', 'Yellow', 'River', '.', 'In', 'the', 'new', 'historicalism', ',', 'in', 'the', '&', 'quot', ';', 'Hometown', 'of', 'the', 'Yellow', 'Flower', '&', 'quot', ';', ',', 'the', 'village', 'committee', 'cadres', 'are', 'not', 'headed', 'for', 'the', 'people', 'like', 'the', 'textbooks', 'of', 'history', '.', 'Instead', ',', 'they', 'ride', 'on', 'the', 'people', '&', '#', '39', ';', 's', 'heads', 'to', 'make', 'a', 'fortune', 'and', 'fight', '.', 'Through', 'the', 'ubiquity', 'of', 'historical', 'figures', 'in', '&', 'quot', ';', 'The', 'Story', 'of', 'Hometown', '&', 'quot', ';', ',', 'Liu', 'Zhenyun', 'also', 'broke', 'the', 'traditional', 'narrative', ',', 'and', 'the', 'meta-fictional', 'narrative', 'technique', 'has', 'stirred', 'up', 'the', 'history', '.', 'At', 'the', 'end', 'of', 'the', 'day', ',', 'it', '’', 's', 'a', 'slap', 'in', 'the', 'word', ',', 'it', '’', 's', 'written', 'in', 'a', 'fascinating', 'way', ',', 'and', 'it', '’', 's', 'nothing', 'more', 'than', 'a', 'slap', 'in', 'the', 'face', '.', 'It', '’', 's', 'nothing', 'more', 'than', 'a', 'ready-made', 'material', '.', 'It', '’', 's', 'nothing', 'more', 'than', 'an', 'attempt', 'to', 'dispel', 'the', 'real', ',', 'blinded', 'review', '.', 'Writing', 'strategies', ',', 'in', 'terms', 'of', 'materials', ',', 'are', 'neither', 'beneficial', 'nor', 'serious', '.', 'Everyone', 'can', 'write', 'novels', ',', 'but', 'not', 'everyone', 'is', 'a', 'novelist', ',', 'and', 'not', 'everyone', 'is', 'a', 'writer', '.', 'Mr.', 'Liu', 'Zhenyun', 'is', 'more', 'willing', 'to', 'put', 'it', 'between', 'the', 'novelist', 'and', 'the', 'writer', '.', 'Once', 'the', 'language', 'of', 'Liu', 'Zhenyun', '&', '#', '39', ';', 's', 'novels', 'is', 'heard', ',', 'everyone', 'can', 'imitate', 'it', '.', 'Secondly', ',', 'Liu', 'Zhenyun', '&', '#', '39', ';', 's', 'delicate', 'observation', 'is', 'unparalleled', '.', 'In', '&', 'quot', ';', 'Wen', 'Wen', '1942', '&', 'quot', ';', ',', 'it', 'is', 'to', 'use', 'data', 'to', 'speak', ',', 'half', 'is', 'like', 'late', 'reportage', ',', 'and', 'in', '&', 'quot', ';', 'Unit', '&', 'quot', ';', ',', '&', 'quot', ';', 'One', 'Place', 'Chicken', '&', 'quot', ';', ',', 'the', 'language', 'of', 'the', 'new', 'realist', 'writer', 'is', 'infinitely', 'close', 'to', 'real', 'life', ',', 'unlimited', 'The', 'degree', 'of', 'closeness', 'to', 'the', 'real', 'life', 'of', 'rice', ',', 'oil', 'and', 'salt', ',', 'unlimited', 'feelings', 'helpless', ',', 'and', 'even', 'a', 'little', 'gloating', '.', 'In', 'the', '&', 'quot', ';', 'Hometown', '&', 'quot', ';', 'series', ',', 'this', 'language', 'has', 'flooded', 'to', 'the', 'extreme', ',', 'and', 'the', 'fart', 'is', 'flying', ',', 'and', 'the', 'spit', 'is', 'smeared', '.', 'Although', 'it', 'likes', 'the', 'extreme', ',', 'the', 'language', '&', '#', '39', ';', 's', 'popularization', 'restricts', 'the', 'theme', 'to', 'a', 'deeper', 'level', '.', 'The', 'biggest', 'characteristic', 'of', 'Liu', 'Zhenyun', 'is', 'that', 'he', 'can', 'go', 'freely', 'in', 'the', 'switching', 'between', 'the', 'microscope', 'and', 'the', 'magnifying', 'glass', '.', 'His', 'microscope', 'is', 'generally', 'meticulously', 'observing', 'life', '.', 'In', 'the', '&', 'quot', ';', 'Unit', '&', 'quot', ';', '&', 'quot', ';', 'The', 'Chicken', 'in', 'a', 'Field', '&', 'quot', ';', ',', 'he', 'is', 'alert', 'to', 'the', 'possibility', 'of', 'a', 'loose', 'political', 'atmosphere', ',', 'and', 'the', 'fledgling', 'The', 'growth', 'of', 'Kobayashi', 'is', 'a', 'true', 'portrayal', 'of', 'the', 'history', 'of', 'this', 'loose', 'atmosphere', '.', 'If', 'a', 'person', 'wants', 'to', 'grow', ',', 'he', 'must', 'be', 'washed', 'by', 'the', 'unit.Practice', 'can', 'only', 'succeed', '.', 'In', 'the', '&', 'quot', ';', 'Hometown', '&', 'quot', ';', 'series', ',', 'he', 'keenly', 'wrote', 'that', 'the', 'subtle', 'relationship', 'between', 'people', 'changes', 'the', 'existence', 'of', 'the', 'hidden', 'class', 'with', 'the', 'difference', 'of', 'economic', 'status', 'and', 'power', 'possession', ',', 'and', 'the', 'story', 'of', '&', 'quot', ';', 'The', 'Hometown', 'Gossip', '&', 'quot', ';', 'from', 'the', 'Three', 'Kingdoms', 'Period', 'The', 'bear-like', 'look', 'turned', 'into', 'a', 'heroic', 'village', 'party', 'secretary', 'for', '59-61', 'years', '.', 'The', 'different', 'attitudes', 'of', 'the', 'dogs', 'are', 'the', 'proof', '.', 'During', 'the', 'great', 'migration', 'period', 'of', 'the', 'Ming', 'Dynasty', ',', 'the', 'people', '’', 's', 'return', 'to', 'the', 'six-finger', 'attitude', 'was', 'also', 'supported', '.', 'Liu', 'Zhenyun', '’', 's', 'magnifying', 'glass', 'tells', 'a', 'weak', ',', 'humble', ',', 'bullying', 'and', 'hard-working', 'nation', 'through', 'these', 'small', 'things', 'that', 'seem', 'to', 'be', 'overwhelmed', 'by', 'historical', 'floods', '.', 'It', 'has', 'always', 'been', 'used', 'in', 'power', 'changes', 'for', 'thousands', 'of', 'years', '.', 'In', '&', 'quot', ';', 'Hometown', 'of', 'the', 'Yellow', 'Flower', '&', 'quot', ';', ',', 'large-scale', 'slogans', 'and', 'policies', 'were', 'replaced', 'by', 'the', 'game', 'of', 'power', '.', 'The', 'power', 'given', 'by', 'the', 'people', 'to', 'be', 'the', 'master', 'of', 'the', 'house', 'was', 'swallowed', 'up', 'by', 'flattery', ',', 'selfishness', ',', 'and', 'numbness', ',', 'for', 'thousands', 'of', 'years', '.', 'I', 'remember', 'listening', 'to', 'Liu', 'Zhenyun', '’', 's', 'speech', 'once', '.', 'He', 'said', 'that', 'he', 'wanted', 'to', 'write', 'the', 'old', 'forest', '30', 'years', 'later', '.', 'Xiaolin', 'was', '30', 'years', 'ago', '.', 'The', 'old', 'forest', 'is', '30', 'years', 'later', ',', 'and', 'people', 'may', 'not', '.', 'He', 'perceives', 'changes', 'in', 'social', 'trends', ',', 'and', 'we', 'will', 'not', 'be', 'remembered', 'by', 'history', '.', 'However', ',', 'changes', 'in', 'history', 'and', 'society', 'will', 'bring', 'about', 'changes', 'in', 'the', 'principles', 'of', 'affairs', 'and', 'interpersonal', 'relationships', '.', 'In', 'the', 'context', 'of', 'small', 'people', ',', 'it', 'is', 'a', 'pity', 'that', 'genes', 'are', 'not', 'necessarily', '.', 'Got', 'the', 'money', '.', 'In', 'a', 'carnival-like', 'discourse', 'magnetic', 'field', 'made', 'up', 'of', 'rough', 'language', ',', 'it', 'is', 'enchanting', 'and', 'fascinating', '.', 'I', 'waited', 'for', 'the', 'heavenly', 'ass', ',', 'the', 'dance', 'of', 'the', 'natural', 'hand', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joE3tJlD8Brc",
        "colab_type": "code",
        "outputId": "352ecc83-50c6-4bd3-8cf7-f661caef1baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#Lemmatize each word in comments\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(token) for token in comments if token.isalpha()]\n",
        "print (lemmatized)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Some', 'of', 'Liu', 'Zhenyun', 's', 'work', 'have', 'been', 'read', 'intermittently', 'The', 'small', 'forest', 'in', 'the', 'Unit', 'and', 'Chicken', 'of', 'the', 'Land', 'grew', 'up', 'under', 'the', 'reform', 'and', 'opening', 'up', 'and', 'the', 'market', 'economy', 'Many', 'historical', 'figure', 'in', 'the', 'Hometown', 'series', 'encircle', 'the', 'historical', 'reality', 'nothing', 'more', 'than', 'Liu', 'Zhenyun', 's', 'black', 'humor', 'The', 'language', 'reveals', 'his', 'embarrassment', 'a', 'a', 'writer', 'The', 'character', 'are', 'not', 'important', 'What', 'is', 'important', 'is', 'to', 'dispel', 'the', 'grand', 'narrative', 'in', 'the', 'historical', 'context', 'to', 'defeat', 'the', 'tall', 'and', 'complete', 'and', 'to', 'return', 'a', 'real', 'name', 'to', 'the', 'world', 'In', 'Liu', 'Zhenyun', 's', 'work', 'you', 'can', 'almost', 'see', 'too', 'many', 'postmodernist', 'technique', 'rather', 'than', 'the', 'pure', 'modernity', 'of', 'the', 'sorrowful', 'magical', 'realism', 'in', 'the', 'quot', 'hometown', 'circulate', 'quot', 'the', 'six', 'finger', 'of', 'the', 'excess', 'finger', 'set', 'up', 'a', 'bridge', 'let', 'Hundreds', 'of', 'thousand', 'of', 'immigrant', 'Yanjin', 'people', 'have', 'successfully', 'crossed', 'the', 'Yellow', 'River', 'In', 'the', 'new', 'historicalism', 'in', 'the', 'quot', 'Hometown', 'of', 'the', 'Yellow', 'Flower', 'quot', 'the', 'village', 'committee', 'cadre', 'are', 'not', 'headed', 'for', 'the', 'people', 'like', 'the', 'textbook', 'of', 'history', 'Instead', 'they', 'ride', 'on', 'the', 'people', 's', 'head', 'to', 'make', 'a', 'fortune', 'and', 'fight', 'Through', 'the', 'ubiquity', 'of', 'historical', 'figure', 'in', 'quot', 'The', 'Story', 'of', 'Hometown', 'quot', 'Liu', 'Zhenyun', 'also', 'broke', 'the', 'traditional', 'narrative', 'and', 'the', 'narrative', 'technique', 'ha', 'stirred', 'up', 'the', 'history', 'At', 'the', 'end', 'of', 'the', 'day', 'it', 's', 'a', 'slap', 'in', 'the', 'word', 'it', 's', 'written', 'in', 'a', 'fascinating', 'way', 'and', 'it', 's', 'nothing', 'more', 'than', 'a', 'slap', 'in', 'the', 'face', 'It', 's', 'nothing', 'more', 'than', 'a', 'material', 'It', 's', 'nothing', 'more', 'than', 'an', 'attempt', 'to', 'dispel', 'the', 'real', 'blinded', 'review', 'Writing', 'strategy', 'in', 'term', 'of', 'material', 'are', 'neither', 'beneficial', 'nor', 'serious', 'Everyone', 'can', 'write', 'novel', 'but', 'not', 'everyone', 'is', 'a', 'novelist', 'and', 'not', 'everyone', 'is', 'a', 'writer', 'Liu', 'Zhenyun', 'is', 'more', 'willing', 'to', 'put', 'it', 'between', 'the', 'novelist', 'and', 'the', 'writer', 'Once', 'the', 'language', 'of', 'Liu', 'Zhenyun', 's', 'novel', 'is', 'heard', 'everyone', 'can', 'imitate', 'it', 'Secondly', 'Liu', 'Zhenyun', 's', 'delicate', 'observation', 'is', 'unparalleled', 'In', 'quot', 'Wen', 'Wen', 'quot', 'it', 'is', 'to', 'use', 'data', 'to', 'speak', 'half', 'is', 'like', 'late', 'reportage', 'and', 'in', 'quot', 'Unit', 'quot', 'quot', 'One', 'Place', 'Chicken', 'quot', 'the', 'language', 'of', 'the', 'new', 'realist', 'writer', 'is', 'infinitely', 'close', 'to', 'real', 'life', 'unlimited', 'The', 'degree', 'of', 'closeness', 'to', 'the', 'real', 'life', 'of', 'rice', 'oil', 'and', 'salt', 'unlimited', 'feeling', 'helpless', 'and', 'even', 'a', 'little', 'gloating', 'In', 'the', 'quot', 'Hometown', 'quot', 'series', 'this', 'language', 'ha', 'flooded', 'to', 'the', 'extreme', 'and', 'the', 'fart', 'is', 'flying', 'and', 'the', 'spit', 'is', 'smeared', 'Although', 'it', 'like', 'the', 'extreme', 'the', 'language', 's', 'popularization', 'restricts', 'the', 'theme', 'to', 'a', 'deeper', 'level', 'The', 'biggest', 'characteristic', 'of', 'Liu', 'Zhenyun', 'is', 'that', 'he', 'can', 'go', 'freely', 'in', 'the', 'switching', 'between', 'the', 'microscope', 'and', 'the', 'magnifying', 'glass', 'His', 'microscope', 'is', 'generally', 'meticulously', 'observing', 'life', 'In', 'the', 'quot', 'Unit', 'quot', 'quot', 'The', 'Chicken', 'in', 'a', 'Field', 'quot', 'he', 'is', 'alert', 'to', 'the', 'possibility', 'of', 'a', 'loose', 'political', 'atmosphere', 'and', 'the', 'fledgling', 'The', 'growth', 'of', 'Kobayashi', 'is', 'a', 'true', 'portrayal', 'of', 'the', 'history', 'of', 'this', 'loose', 'atmosphere', 'If', 'a', 'person', 'want', 'to', 'grow', 'he', 'must', 'be', 'washed', 'by', 'the', 'can', 'only', 'succeed', 'In', 'the', 'quot', 'Hometown', 'quot', 'series', 'he', 'keenly', 'wrote', 'that', 'the', 'subtle', 'relationship', 'between', 'people', 'change', 'the', 'existence', 'of', 'the', 'hidden', 'class', 'with', 'the', 'difference', 'of', 'economic', 'status', 'and', 'power', 'possession', 'and', 'the', 'story', 'of', 'quot', 'The', 'Hometown', 'Gossip', 'quot', 'from', 'the', 'Three', 'Kingdoms', 'Period', 'The', 'look', 'turned', 'into', 'a', 'heroic', 'village', 'party', 'secretary', 'for', 'year', 'The', 'different', 'attitude', 'of', 'the', 'dog', 'are', 'the', 'proof', 'During', 'the', 'great', 'migration', 'period', 'of', 'the', 'Ming', 'Dynasty', 'the', 'people', 's', 'return', 'to', 'the', 'attitude', 'wa', 'also', 'supported', 'Liu', 'Zhenyun', 's', 'magnifying', 'glass', 'tell', 'a', 'weak', 'humble', 'bullying', 'and', 'nation', 'through', 'these', 'small', 'thing', 'that', 'seem', 'to', 'be', 'overwhelmed', 'by', 'historical', 'flood', 'It', 'ha', 'always', 'been', 'used', 'in', 'power', 'change', 'for', 'thousand', 'of', 'year', 'In', 'quot', 'Hometown', 'of', 'the', 'Yellow', 'Flower', 'quot', 'slogan', 'and', 'policy', 'were', 'replaced', 'by', 'the', 'game', 'of', 'power', 'The', 'power', 'given', 'by', 'the', 'people', 'to', 'be', 'the', 'master', 'of', 'the', 'house', 'wa', 'swallowed', 'up', 'by', 'flattery', 'selfishness', 'and', 'numbness', 'for', 'thousand', 'of', 'year', 'I', 'remember', 'listening', 'to', 'Liu', 'Zhenyun', 's', 'speech', 'once', 'He', 'said', 'that', 'he', 'wanted', 'to', 'write', 'the', 'old', 'forest', 'year', 'later', 'Xiaolin', 'wa', 'year', 'ago', 'The', 'old', 'forest', 'is', 'year', 'later', 'and', 'people', 'may', 'not', 'He', 'perceives', 'change', 'in', 'social', 'trend', 'and', 'we', 'will', 'not', 'be', 'remembered', 'by', 'history', 'However', 'change', 'in', 'history', 'and', 'society', 'will', 'bring', 'about', 'change', 'in', 'the', 'principle', 'of', 'affair', 'and', 'interpersonal', 'relationship', 'In', 'the', 'context', 'of', 'small', 'people', 'it', 'is', 'a', 'pity', 'that', 'gene', 'are', 'not', 'necessarily', 'Got', 'the', 'money', 'In', 'a', 'discourse', 'magnetic', 'field', 'made', 'up', 'of', 'rough', 'language', 'it', 'is', 'enchanting', 'and', 'fascinating', 'I', 'waited', 'for', 'the', 'heavenly', 'as', 'the', 'dance', 'of', 'the', 'natural', 'hand']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv85JL2L-AAv",
        "colab_type": "code",
        "outputId": "29977f0f-c7b6-4102-d948-ca98557f8ba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "#Remove punctuation and stop words\n",
        "stopwords_removed = [token for token in lemmatized if not token in stopwords.words('english')]\n",
        "print (stopwords_removed)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Some', 'Liu', 'Zhenyun', 'work', 'read', 'intermittently', 'The', 'small', 'forest', 'Unit', 'Chicken', 'Land', 'grew', 'reform', 'opening', 'market', 'economy', 'Many', 'historical', 'figure', 'Hometown', 'series', 'encircle', 'historical', 'reality', 'nothing', 'Liu', 'Zhenyun', 'black', 'humor', 'The', 'language', 'reveals', 'embarrassment', 'writer', 'The', 'character', 'important', 'What', 'important', 'dispel', 'grand', 'narrative', 'historical', 'context', 'defeat', 'tall', 'complete', 'return', 'real', 'name', 'world', 'In', 'Liu', 'Zhenyun', 'work', 'almost', 'see', 'many', 'postmodernist', 'technique', 'rather', 'pure', 'modernity', 'sorrowful', 'magical', 'realism', 'quot', 'hometown', 'circulate', 'quot', 'six', 'finger', 'excess', 'finger', 'set', 'bridge', 'let', 'Hundreds', 'thousand', 'immigrant', 'Yanjin', 'people', 'successfully', 'crossed', 'Yellow', 'River', 'In', 'new', 'historicalism', 'quot', 'Hometown', 'Yellow', 'Flower', 'quot', 'village', 'committee', 'cadre', 'headed', 'people', 'like', 'textbook', 'history', 'Instead', 'ride', 'people', 'head', 'make', 'fortune', 'fight', 'Through', 'ubiquity', 'historical', 'figure', 'quot', 'The', 'Story', 'Hometown', 'quot', 'Liu', 'Zhenyun', 'also', 'broke', 'traditional', 'narrative', 'narrative', 'technique', 'ha', 'stirred', 'history', 'At', 'end', 'day', 'slap', 'word', 'written', 'fascinating', 'way', 'nothing', 'slap', 'face', 'It', 'nothing', 'material', 'It', 'nothing', 'attempt', 'dispel', 'real', 'blinded', 'review', 'Writing', 'strategy', 'term', 'material', 'neither', 'beneficial', 'serious', 'Everyone', 'write', 'novel', 'everyone', 'novelist', 'everyone', 'writer', 'Liu', 'Zhenyun', 'willing', 'put', 'novelist', 'writer', 'Once', 'language', 'Liu', 'Zhenyun', 'novel', 'heard', 'everyone', 'imitate', 'Secondly', 'Liu', 'Zhenyun', 'delicate', 'observation', 'unparalleled', 'In', 'quot', 'Wen', 'Wen', 'quot', 'use', 'data', 'speak', 'half', 'like', 'late', 'reportage', 'quot', 'Unit', 'quot', 'quot', 'One', 'Place', 'Chicken', 'quot', 'language', 'new', 'realist', 'writer', 'infinitely', 'close', 'real', 'life', 'unlimited', 'The', 'degree', 'closeness', 'real', 'life', 'rice', 'oil', 'salt', 'unlimited', 'feeling', 'helpless', 'even', 'little', 'gloating', 'In', 'quot', 'Hometown', 'quot', 'series', 'language', 'ha', 'flooded', 'extreme', 'fart', 'flying', 'spit', 'smeared', 'Although', 'like', 'extreme', 'language', 'popularization', 'restricts', 'theme', 'deeper', 'level', 'The', 'biggest', 'characteristic', 'Liu', 'Zhenyun', 'go', 'freely', 'switching', 'microscope', 'magnifying', 'glass', 'His', 'microscope', 'generally', 'meticulously', 'observing', 'life', 'In', 'quot', 'Unit', 'quot', 'quot', 'The', 'Chicken', 'Field', 'quot', 'alert', 'possibility', 'loose', 'political', 'atmosphere', 'fledgling', 'The', 'growth', 'Kobayashi', 'true', 'portrayal', 'history', 'loose', 'atmosphere', 'If', 'person', 'want', 'grow', 'must', 'washed', 'succeed', 'In', 'quot', 'Hometown', 'quot', 'series', 'keenly', 'wrote', 'subtle', 'relationship', 'people', 'change', 'existence', 'hidden', 'class', 'difference', 'economic', 'status', 'power', 'possession', 'story', 'quot', 'The', 'Hometown', 'Gossip', 'quot', 'Three', 'Kingdoms', 'Period', 'The', 'look', 'turned', 'heroic', 'village', 'party', 'secretary', 'year', 'The', 'different', 'attitude', 'dog', 'proof', 'During', 'great', 'migration', 'period', 'Ming', 'Dynasty', 'people', 'return', 'attitude', 'wa', 'also', 'supported', 'Liu', 'Zhenyun', 'magnifying', 'glass', 'tell', 'weak', 'humble', 'bullying', 'nation', 'small', 'thing', 'seem', 'overwhelmed', 'historical', 'flood', 'It', 'ha', 'always', 'used', 'power', 'change', 'thousand', 'year', 'In', 'quot', 'Hometown', 'Yellow', 'Flower', 'quot', 'slogan', 'policy', 'replaced', 'game', 'power', 'The', 'power', 'given', 'people', 'master', 'house', 'wa', 'swallowed', 'flattery', 'selfishness', 'numbness', 'thousand', 'year', 'I', 'remember', 'listening', 'Liu', 'Zhenyun', 'speech', 'He', 'said', 'wanted', 'write', 'old', 'forest', 'year', 'later', 'Xiaolin', 'wa', 'year', 'ago', 'The', 'old', 'forest', 'year', 'later', 'people', 'may', 'He', 'perceives', 'change', 'social', 'trend', 'remembered', 'history', 'However', 'change', 'history', 'society', 'bring', 'change', 'principle', 'affair', 'interpersonal', 'relationship', 'In', 'context', 'small', 'people', 'pity', 'gene', 'necessarily', 'Got', 'money', 'In', 'discourse', 'magnetic', 'field', 'made', 'rough', 'language', 'enchanting', 'fascinating', 'I', 'waited', 'heavenly', 'dance', 'natural', 'hand']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BnBl_Rj-aUf",
        "colab_type": "code",
        "outputId": "53a447cf-8b0a-4b48-e7c3-32d331cedff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "#Create TF-IDF Vectors\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=3)\n",
        "vectorizer.fit(stopwords_removed)\n",
        "\n",
        "vector = vectorizer.transform(stopwords_removed)\n",
        "\n",
        "\n",
        "print(vector.toarray())\n",
        "print('\\n')\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            "\n",
            "{'liu': 13, 'zhenyun': 29, 'the': 22, 'small': 21, 'forest': 3, 'unit': 24, 'chicken': 1, 'historical': 5, 'hometown': 7, 'series': 20, 'nothing': 15, 'language': 10, 'writer': 26, 'narrative': 14, 'real': 19, 'in': 8, 'quot': 18, 'thousand': 23, 'people': 16, 'yellow': 28, 'like': 12, 'history': 6, 'ha': 4, 'it': 9, 'everyone': 2, 'life': 11, 'change': 0, 'power': 17, 'year': 27, 'wa': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bfgo3CC-lVJ",
        "colab_type": "code",
        "outputId": "e0d097c7-8917-489b-9795-f4b08d2ee00d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "#POS\n",
        "# Tag each word from output one with Part of Speech\n",
        "\n",
        "comments\n",
        "\n",
        "POS = []\n",
        "for comment in comments:\n",
        "    tokens = nltk.word_tokenize(comment) # tokenize the words\n",
        "    POS_tokens = nltk.pos_tag(tokens) # pos tag the tokens\n",
        "    POS_token_temp = []\n",
        "    for i in POS_tokens:\n",
        "        POS_token_temp.append(i[0] + i[1]) # create tuples for the tokens/pos tags\n",
        "    POS.append(\" \".join(POS_token_temp))\n",
        "\n",
        "vectorizer2 = TfidfVectorizer()\n",
        "vectorizer2.fit(POS)\n",
        "print(vectorizer2.vocabulary_)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "POS_2 = vectorizer2.transform(POS)\n",
        "print(POS_2.toarray())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'somedt': 280, 'ofin': 208, 'liunn': 167, 'zhenyunnn': 356, 'nn': 199, 'snn': 277, 'worksnns': 344, 'havevb': 118, 'beenvbn': 25, 'readnn': 239, 'intermittentlyrb': 145, 'thedt': 303, 'smalljj': 275, 'forestnn': 95, 'inin': 143, 'unitnn': 321, 'andcc': 14, 'chickennn': 45, 'landnn': 154, 'grewvbd': 111, 'uprb': 324, 'underin': 319, 'reformnn': 245, 'openingnn': 215, 'marketnn': 177, 'economynn': 69, 'manyjj': 176, 'historicaljj': 129, 'figuresnns': 86, 'hometownnnp': 132, 'seriesnn': 268, 'encirclenn': 72, 'realitynn': 243, 'nothingnn': 201, 'morerbr': 189, 'thanin': 301, 'blackjj': 30, 'humornn': 136, 'languagenn': 155, 'revealsnns': 254, 'hisprp': 127, 'embarrassmentnn': 70, 'asin': 17, 'adt': 6, 'writernn': 347, 'charactersnns': 44, 'arevbp': 16, 'notrb': 202, 'importantjj': 141, 'whatwp': 338, 'isvbz': 149, 'toto': 313, 'dispelnn': 64, 'grandjj': 109, 'narrativejj': 193, 'contextnn': 52, 'defeatnn': 58, 'tallnn': 295, 'completejj': 51, 'returnnn': 253, 'realjj': 244, 'namenn': 192, 'worldnn': 345, 'cc': 41, '39cd': 2, 'youprp': 355, 'canmd': 39, 'almostrb': 10, 'seevb': 266, 'toorb': 312, 'postmodernistnn': 230, 'techniquesnns': 297, 'ratherrb': 238, 'purenn': 235, 'modernitynn': 187, 'sorrowfulnn': 281, 'magicaljj': 172, 'realismnn': 241, 'quotnn': 237, 'hometownnn': 131, 'circulatenn': 46, 'sixcd': 272, 'fingersnns': 88, 'excessnn': 76, 'setnn': 270, 'bridgenn': 32, 'letvb': 159, 'hundredsnns': 137, 'thousandsnns': 309, 'immigrantnn': 140, 'yanjinnn': 352, 'peoplenns': 218, 'successfullyrb': 291, 'crossedvbn': 53, 'yellowvb': 354, 'rivernn': 258, 'newjj': 198, 'historicalismnn': 128, 'flowernn': 93, 'villagenn': 327, 'committeenn': 50, 'cadresnns': 38, 'headedvbn': 119, 'forin': 96, 'likein': 162, 'textbooksnns': 300, 'historynn': 130, 'insteadrb': 144, 'theyprp': 306, 'ridenn': 257, 'onin': 213, 'headsnns': 120, 'makevb': 175, 'fortunenn': 97, 'fightnn': 85, 'throughin': 311, 'ubiquitynn': 318, 'storynn': 287, 'alsorb': 11, 'brokenn': 34, 'traditionaljj': 314, 'meta': 182, 'fictionaljj': 83, 'techniquenn': 296, 'hasvbz': 117, 'stirredvbn': 286, 'atin': 19, 'endnn': 73, 'daynn': 56, 'itprp': 150, 'slapnn': 273, 'wordnn': 342, 'writtenvbn': 349, 'fascinatingvbg': 81, 'waynn': 333, 'facenn': 79, 'ready': 240, 'madenn': 170, 'materialnn': 179, 'andt': 15, 'attemptnn': 21, 'blindedvbn': 31, 'reviewnn': 255, 'writingvbg': 348, 'strategiesnns': 288, 'termsnns': 299, 'materialsnns': 180, 'neitherdt': 197, 'beneficialjj': 26, 'norcc': 200, 'seriousjj': 269, 'everyonenn': 75, 'writenn': 346, 'novelsnns': 204, 'butcc': 36, 'novelistnn': 203, 'mrnnp': 190, 'willingjj': 339, 'putnn': 236, 'betweenin': 27, 'oncerb': 211, 'heardnn': 121, 'imitatenn': 139, 'secondlyrb': 263, 'delicatenn': 60, 'observationnn': 206, 'unparalleledjj': 323, 'wennn': 335, '1942cd': 0, 'usenn': 326, 'datanns': 55, 'speaknn': 282, 'halfnn': 114, 'laterb': 157, 'reportagenn': 251, 'onecd': 212, 'placenn': 223, 'realistnn': 242, 'infinitelyrb': 142, 'closerb': 49, 'lifenn': 161, 'unlimitedjj': 322, 'degreenn': 59, 'closenessnn': 48, 'ricenn': 256, 'oilnn': 209, 'saltnn': 261, 'feelingsnns': 82, 'helplessnn': 123, 'evenrb': 74, 'littlejj': 166, 'gloatingvbg': 105, 'thisdt': 308, 'floodedvbn': 91, 'extremenn': 78, 'fartnn': 80, 'flyingvbg': 94, 'spitnn': 284, 'smearedvbn': 276, 'althoughin': 12, 'likesnns': 164, 'popularizationnn': 226, 'restrictsnns': 252, 'themenn': 304, 'deepernn': 57, 'levelnn': 160, 'biggestjjs': 29, 'characteristicjj': 43, 'thatin': 302, 'heprp': 124, 'govb': 108, 'freelyrb': 98, 'switchingvbg': 294, 'microscopenn': 184, 'magnifyingvbg': 174, 'glassnn': 104, 'generallyrb': 101, 'meticulouslyrb': 183, 'observingvbg': 207, 'fieldnn': 84, 'alertnn': 9, 'possibilitynn': 229, 'loosejj': 169, 'politicaljj': 225, 'atmosphererb': 20, 'fledglingnn': 90, 'growthnn': 113, 'kobayashinnp': 153, 'truejj': 316, 'portrayalnn': 227, 'ifin': 138, 'personnn': 221, 'wantsvbz': 330, 'grownn': 112, 'mustmd': 191, 'bevb': 28, 'washedvbn': 331, 'byin': 37, 'unit': 320, 'practicenn': 232, 'onlyrb': 214, 'succeedvb': 290, 'keenlyrb': 151, 'wrotevbd': 350, 'subtlenn': 289, 'relationshipnn': 246, 'changesnns': 42, 'existencenn': 77, 'hiddennn': 126, 'classnn': 47, 'within': 341, 'differencenn': 61, 'economicjj': 68, 'statusnn': 285, 'powernn': 231, 'possessionnn': 228, 'gossipnn': 106, 'fromin': 99, 'threecd': 310, 'kingdomsnns': 152, 'periodnn': 220, 'bear': 24, 'likenn': 163, 'looknn': 168, 'turnedvbd': 317, 'intoin': 147, 'heroicnn': 125, 'partynn': 217, 'secretarynn': 264, '59': 3, '61jj': 4, 'yearsnns': 353, 'differentjj': 62, 'attitudesnns': 23, 'dogsnns': 65, 'proofnn': 234, 'duringin': 66, 'greatjj': 110, 'migrationnn': 185, 'mingvbg': 186, 'dynastynn': 67, 'six': 271, 'fingernn': 87, 'attitudenn': 22, 'wasvbd': 332, 'supportedvbn': 292, 'tellsnns': 298, 'weakjj': 334, 'humblejj': 135, 'bullyingnn': 35, 'hard': 116, 'workingnn': 343, 'nationnn': 194, 'thesedt': 305, 'thingsnns': 307, 'seemnn': 265, 'overwhelmedjj': 216, 'floodsnns': 92, 'alwaysrb': 13, 'usedvbn': 325, 'large': 156, 'scalejj': 262, 'slogansnns': 274, 'policiesnns': 224, 'werevbd': 337, 'replacedvbn': 250, 'gamenn': 100, 'givenvbn': 103, 'masternn': 178, 'housenn': 133, 'swallowedvbn': 293, 'flatterynn': 89, 'selfishnessnn': 267, 'numbnessnn': 205, 'iprp': 148, 'remembervb': 249, 'listeningvbg': 165, 'speechnn': 283, 'saidvbd': 260, 'wantedvbd': 329, 'oldjj': 210, '30cd': 1, 'laterrb': 158, 'xiaolinnn': 351, 'agorb': 8, 'maymd': 181, 'perceivesnns': 219, 'socialjj': 278, 'trendsnns': 315, 'weprp': 336, 'willmd': 340, 'rememberedvbn': 248, 'howeverrb': 134, 'societynn': 279, 'bringnn': 33, 'aboutin': 5, 'principlesnns': 233, 'affairsnns': 7, 'interpersonaljj': 146, 'relationshipsnns': 247, 'pitynn': 222, 'genesnns': 102, 'necessarilyrb': 196, 'gotnnp': 107, 'moneynn': 188, 'carnival': 40, 'discoursenn': 63, 'magneticjj': 173, 'madevbn': 171, 'roughnn': 259, 'enchantingvbg': 71, 'waitedvbd': 328, 'heavenlyrb': 122, 'assnn': 18, 'dancenn': 54, 'naturaljj': 195, 'handnn': 115}\n",
            "\n",
            "\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSmMc3fa-qke",
        "colab_type": "text"
      },
      "source": [
        "Part 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNmTejck-tW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import\n",
        "from google.colab import files\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pylab import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TF_ZPJF_BQK",
        "colab_type": "code",
        "outputId": "37c17cbd-279e-40bc-868b-de0699f887fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "#loop through and add each image to a list\n",
        "# for loop to grab all of the files from the current working directory\n",
        "\n",
        "#create a list to store png images in\n",
        "PNGImages = []\n",
        "for img_path in os.listdir():\n",
        "  # PNG files\n",
        "  if img_path.endswith(\".PNG\"):\n",
        "      #reading images\n",
        "      img = Image.open(img_path)\n",
        "      #convert to grayscale\n",
        "      img = img.convert('L')\n",
        "      #resize 100 x 100 pixels\n",
        "      img = np.resize(im,(100,100,3))\n",
        "      #flatten to a 1D array\n",
        "      img = img.flatten()\n",
        "      #append each image to the list\n",
        "      PNGImages.append(img)\n",
        "\n",
        "\n",
        "# plotting the image arrays in a histogram with 256 bins\n",
        "plt.hist(PNGImages,bins=255,stacked=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ab9ce9972333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;31m#resize 100 x 100 pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0;31m#flatten to a 1D array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'im' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfj55N4iAtyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a new list\n",
        "PNGImages_2 =[]\n",
        "# for loop to grab all of the files from the current working directory\n",
        "for img_path in os.listdir():\n",
        "  # grabbing all PNG files\n",
        "  if img_path.endswith(\".PNG\"):\n",
        "      #reading in the images\n",
        "      img = Image.open(img_path)\n",
        "      #converting them to grayscale\n",
        "      img = img.convert('L')\n",
        "      #resizing them to 100 x 100 pixels\n",
        "      img = np.resize(im,(100,100,3))\n",
        "      #flattening the 2D array to a 1D array\n",
        "      img = img.flatten()\n",
        "      img, bins = histogram(img, 256, density=True)\n",
        "      img = img.cumsum()\n",
        "      img = 255 * img / img[-1] # Normalization to ensure that we are in [0,255]\n",
        "      #appending the image arrays to the list I initiated earlier\n",
        "      PNGImages_2.append(img)\n",
        "\n",
        "# plotting the image arrays in a histogram with 256 bins\n",
        "plt.hist(PNGImages_2, bins=255, stacked=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}